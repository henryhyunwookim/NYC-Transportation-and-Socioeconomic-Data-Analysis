{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all requisite libraries, settings some overall parameters and formatting.\n",
    "%reset -f\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "#from pandas_profiling import ProfileReport\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_rows = 20\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set_style('darkgrid');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCharactersFromStationName(df):\n",
    "    # Remove special characters from station names\n",
    "    df.station = df.station.str.replace(\"/\",\"_\")\n",
    "    df.station = df.station.str.replace(\"-\",\"_\")\n",
    "    df.station = df.station.str.replace(\" \",\"_\")\n",
    "    df.station = df.station.str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCharactersFromStationTimestamps(df):\n",
    "    # Remove special characters from station dates\n",
    "    df['date'] = df['date'].str.replace('/','_')\n",
    "    df['time'] = df['time'].str.replace(':','_')\n",
    "    df['desc'] = df['desc'].str.replace(' ', '_')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createStationCountsByTime(df, input_col, col_name='entry'):\n",
    "\n",
    "    df[col_name+'_day'] = df[input_col].dt.day\n",
    "    df[col_name+'_hour'] = df[input_col].dt.hour\n",
    "    df[col_name+'_weekday'] = df[input_col].dt.day_name()\n",
    "    df[col_name+'_year_month'] = df[input_col].dt.to_period('M')\n",
    "    print(f'\"{input_col}\" splitted into multiple columns.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTrafficRidershipCounts(df):\n",
    "    # Creating the Net_Entries, Net_Exits, and Net_Traffic columns\n",
    "    df['net_entries'] = df.groupby(['control_area', 'unit', 'subunit_channel_pos', 'station'])['entries'].transform(lambda x: x.diff())\n",
    "    df['net_exits'] = df.groupby(['control_area', 'unit', 'subunit_channel_pos', 'station'])['exits'].transform(lambda x: x.diff())\n",
    "    df['net_traffic'] = df.net_entries + df.net_exits\n",
    "\n",
    "    # Elimating turnstiles that count in reverse by casting all values as absolutes.\n",
    "    df['net_entries'] = abs(df.net_entries)\n",
    "    df['net_exits'] = abs(df.net_exits)\n",
    "    df['net_traffic'] = abs(df.net_traffic)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(df):\n",
    "    # Elimate outliers in the data by reducing to the 99th percentile. \n",
    "    q = np.nanquantile(df[\"net_entries\"], .99)\n",
    "    df = df[df[\"net_entries\"] < q]\n",
    "\n",
    "    q2 = np.nanquantile(df[\"net_exits\"], .99)\n",
    "    df = df[df[\"net_exits\"] < q2]\n",
    "\n",
    "    q3 = np.nanquantile(df['net_traffic'], .99)\n",
    "    df=df[df['net_traffic'] < q3]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCharactersFromStopName(df):\n",
    "    df.stop_name = df.stop_name.str.replace(\" - \",\"_\")\n",
    "    df.stop_name = df.stop_name.str.replace(\" \",\"_\")\n",
    "    df.stop_name = df.stop_name.str.replace(\"(\",\"\")\n",
    "    df.stop_name = df.stop_name.str.replace(\")\",\"\")\n",
    "    df.stop_name = df.stop_name.str.replace(\"/\",\"_\")\n",
    "    df.stop_name = df.stop_name.str.replace(\".\",\"\")\n",
    "    df.stop_name = df.stop_name.str.replace(\"-\",\"_\")\n",
    "    df.stop_name = df.stop_name.str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchStationNames(df,df_gtfs):\n",
    "    \n",
    "    mat1 = []\n",
    "    mat2 = []\n",
    "    p= []\n",
    "    list1 = df.station.tolist()\n",
    "    list2 = df_gtfs.stop_name.tolist()\n",
    " \n",
    "    threshold = 50\n",
    "\n",
    "    for i in list1:\n",
    "        mat1.append(process.extractOne(i, list2, scorer=fuzz.ratio))\n",
    "    df['matches'] = mat1\n",
    "\n",
    "    for j in df['matches']:\n",
    "        if j[1] >= threshold:\n",
    "            p.append(j[0])\n",
    "\n",
    "        mat2.append(','.join(p))\n",
    "        p= []\n",
    "\n",
    "    df['matches'] = mat2\n",
    "    return df,df_gtfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rtree\n",
    "import pygeos\n",
    "def combineGTFSStopsAndStationData(df):\n",
    "   \n",
    "    df['geometry'] = [Point(xy) for xy in zip(np.array(df['gtfs_longitude']), np.array(df['gtfs_latitude']))]\n",
    "    gpd.options.use_pygeos = True\n",
    "    \n",
    "    cdta_map = gpd.read_file(\"..\\\\data\\\\nycdta2020_22b\\\\nycdta2020.shp\")\n",
    "    cdta_map.to_crs(4326, inplace=True)\n",
    "    \n",
    "    cdta_geo_df = cdta_map[['CDTA2020', 'CDTAName','geometry', 'Shape_Leng', 'Shape_Area','BoroName']].set_index('CDTA2020', drop=True)\n",
    "    \n",
    "    top_station_geo_df = gpd.GeoDataFrame(df, crs=4326, geometry = df.geometry)\n",
    "    top_station_geo_df.to_crs(4326, inplace=True)\n",
    "    \n",
    "    #df.to_csv('allstation1.csv')\n",
    "    # Locate each Station Point Geometry within NTA Polygon geometry\n",
    "    station_all_df = gpd.sjoin(cdta_geo_df,top_station_geo_df, how=\"left\", op=\"contains\")\n",
    "    station_all_df = station_all_df.reset_index()\n",
    "    \n",
    "    #print('before ctda cleanup')\n",
    "    #print(station_all_df.head(1))\n",
    "    #print(station_all_df.columns)\n",
    "    #print(station_all_df.shape)\n",
    "    \n",
    "    station_all_df = station_all_df[station_all_df['CDTA2020'].str.match('^[a-zA-Z]{2}\\d{2}$')]\n",
    "    \n",
    "    #print('after ctda cleanup')\n",
    "    #print(station_all_df.head(1))\n",
    "    #print(station_all_df.columns)\n",
    "    #print(station_all_df.shape)\n",
    "    \n",
    "    #Few stations that belong to Manhattan Burough were identified based on the CDTA code\n",
    "    station_all_df['borough'] = station_all_df.borough.fillna(\"M\")\n",
    "    \n",
    "    print('combineGTFSStopsAndStationData ouput..')\n",
    "    print(station_all_df.columns)   \n",
    "    print(station_all_df.head(1))\n",
    "    cdta_dict = cdta_map[[\"CDTA2020\", \"CDTAName\"]].set_index(\"CDTA2020\").to_dict()[\"CDTAName\"]\n",
    "    return station_all_df,cdta_dict   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load first six months of 2022 Ridership data for subway stations using Turnstile datasource.\n",
    "\n",
    "engine = create_engine(\"sqlite:///C:\\\\Users\\\\panch\\\\capstone\\\\notebooks\\\\mta_data.db\")\n",
    "mta_df = pd.read_sql('SELECT * FROM mta_data;', engine)\n",
    "\n",
    "#Cleanup data from station names\n",
    "# Rename mta_df columns to make them easier to work wit\n",
    "mta_df = mta_df.rename(columns={'C/A': 'control_area', 'UNIT': 'unit', 'SCP': 'subunit_channel_pos', 'STATION':'station', 'LINENAME':'subway_lines', 'DIVISION':'division', 'DATE':'date', 'TIME':'time', 'DESC':'desc', 'ENTRIES':'entries', 'EXITS':'exits'})\n",
    "mta_df = removeSpecialCharactersFromStationName(mta_df)\n",
    "mta_df = removeSpecialCharactersFromStationTimestamps(mta_df)\n",
    "\n",
    "mta_df['subunit_channel_pos'] = mta_df['subunit_channel_pos'].str.replace('-', '_')\n",
    "\n",
    "# Create UniqueId column for grouping by \n",
    "mta_df['unique_id'] = mta_df['control_area'] + '_' + mta_df['unit'] + '_' + mta_df['subunit_channel_pos'] + '_' + mta_df['station'] + '_' + mta_df['date'] + '_' + mta_df['time'] + '_' + mta_df['desc']\n",
    "mta_df['date_time'] = mta_df.date + ' ' + mta_df.time\n",
    "mta_df.date_time = pd.to_datetime(mta_df['date_time'], format = '%m_%d_%Y %H_%M_%S')\n",
    "mta_df = mta_df[mta_df.desc != 'RECOVR_AUD']\n",
    "\n",
    "mta_df = computeTrafficRidershipCounts(mta_df)\n",
    "mta_df.fillna(0, inplace=True)\n",
    "\n",
    "# Elimate outliers in the data by reducing to the 99th percentile. \n",
    "mta_df = removeOutliers(mta_df)\n",
    "\n",
    "mta_df = createStationCountsByTime(mta_df, 'date_time', col_name='Station_Readings_Entry')\n",
    "mta_df = createStationCountsByTime(mta_df, 'date_time', col_name='Station_Readings_Exit')\n",
    "\n",
    "print('Step 1..')\n",
    "print(mta_df.columns)\n",
    "print(mta_df.shape)\n",
    "print(mta_df.head(10))\n",
    "\n",
    "print('Stations With passenger counts and timestamps ,net entries and net exits and net traffic(which is sum of net entry and exit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#2. Load GTFS stop data for mapping Borough and CDTA using Lat and Long geometry to station data.\n",
    "cols_to_keep = [\"station\",\"entries\", \"exits\",\"net_entries\",\"net_exits\",\"net_traffic\",\"Station_Readings_Exit_weekday\",\"Station_Readings_Exit_year_month\",\"Station_Readings_Exit_day\",\"Station_Readings_Exit_hour\",\"Station_Readings_Entry_weekday\",\"Station_Readings_Entry_year_month\",\"Station_Readings_Entry_day\",\"Station_Readings_Entry_hour\"]\n",
    "group_cols = [\"net_traffic\"]\n",
    "\n",
    "station_data_gtfs = pd.read_csv('../data/subway/stationnames.csv')\n",
    "station_data_gtfs = removeSpecialCharactersFromStopName(station_data_gtfs)\n",
    "\n",
    "mta_df = mta_df[cols_to_keep]\n",
    "                \n",
    "top_stations = mta_df.groupby('station')[group_cols].sum().sort_values(by='net_traffic', ascending=False).reset_index().copy()\n",
    "\n",
    "top_stations,station_data_gtfs = matchStationNames(top_stations,station_data_gtfs)\n",
    "\n",
    "#Merge station names from GTFS and Turnstile data for mapping CDTA\n",
    "top_station_df = pd.merge(top_stations, right=station_data_gtfs, left_on='matches', right_on='stop_name', how='left')\n",
    "\n",
    "#Merge CDTA Code and Burough code into single DF\n",
    "stationWithCdta,cdta_dict = combineGTFSStopsAndStationData(top_station_df)\n",
    "stationWithCdta.dropna(subset=['station'], how='all', inplace=True)\n",
    "stationWithCdta.info() \n",
    "\n",
    "stationWithCdta = stationWithCdta.rename(columns={'CDTA2020': 'cdtaCode'})\n",
    "\n",
    "#Create CDTA Dictionary\n",
    "cdta_station_dict = stationWithCdta[[\"cdtaCode\", \"station\"]].set_index(\"station\").to_dict()[\"cdtaCode\"]\n",
    "#print(cdta_station_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate CDTA NetEntries and NetExits\n",
    "print(mta_df.columns)\n",
    "net_entry_stations = mta_df.groupby(['station'])[\"net_entries\"].sum().reset_index().copy()\n",
    "net_exit_stations = mta_df.groupby(['station'])[\"net_exits\"].sum().reset_index().copy()\n",
    "\n",
    "net_entry_stations['cdtaCode_entry'] = net_entry_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "net_exit_stations['cdtaCode_exit'] = net_entry_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "\n",
    "net_entry_cdta = net_entry_stations.groupby(['cdtaCode_entry']).sum()\n",
    "net_exit_cdta = net_exit_stations.groupby(['cdtaCode_exit']).sum()\n",
    "\n",
    "print(net_entry_cdta.shape)\n",
    "print(net_exit_cdta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate CDTA Day wise (1 to 31 ) NetEntries and NetExits\n",
    "\n",
    "print(mta_df.columns)\n",
    "day_entry_stations = mta_df.groupby(['Station_Readings_Entry_day','station'])[\"net_entries\"].sum().reset_index().copy()\n",
    "day_exit_stations = mta_df.groupby(['Station_Readings_Exit_day','station'])[\"net_exits\"].sum().reset_index().copy()\n",
    "\n",
    "day_entry_stations['cdtaCode_entry'] = day_entry_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "day_exit_stations['cdtaCode_exit'] = day_exit_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "\n",
    "## Entry day count by CDTA\n",
    "entry_day_count_cdta = day_entry_stations.groupby(['cdtaCode_entry', 'Station_Readings_Entry_day']).sum().reset_index()\\\n",
    "    [['cdtaCode_entry', 'Station_Readings_Entry_day','net_entries']].pivot(index='cdtaCode_entry', columns=\"Station_Readings_Entry_day\", values=\"net_entries\")\n",
    "entry_day_count_cdta.columns = [\"Entry_total_trip_count_day_\" + str(col) for col in entry_day_count_cdta.columns]\n",
    "\n",
    "## Exit day count by CDTA\n",
    "exit_day_count_cdta = day_exit_stations.groupby(['cdtaCode_exit', 'Station_Readings_Exit_day']).sum().reset_index()\\\n",
    "    [['cdtaCode_exit', 'Station_Readings_Exit_day','net_exits']].pivot(index='cdtaCode_exit', columns=\"Station_Readings_Exit_day\", values=\"net_exits\")\n",
    "exit_day_count_cdta.columns = [\"Exit_total_trip_count_day_\" + str(col) for col in exit_day_count_cdta.columns]\n",
    "\n",
    "print(entry_day_count_cdta.shape)\n",
    "print(exit_day_count_cdta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Calculate CDTA Hour wise (1 to 24 ) NetEntries and NetExits\n",
    "\n",
    "print(mta_df.columns)\n",
    "hour_entry_stations = mta_df.groupby(['Station_Readings_Entry_hour','station'])[\"net_entries\"].sum().reset_index().copy()\n",
    "hour_exit_stations = mta_df.groupby(['Station_Readings_Exit_hour','station'])[\"net_exits\"].sum().reset_index().copy()\n",
    "\n",
    "hour_entry_stations['cdtaCode_entry'] = hour_entry_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "hour_exit_stations['cdtaCode_exit'] = hour_exit_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "\n",
    " ## Entry Hour count by CDTA\n",
    "entry_hour_count_cdta = hour_entry_stations.groupby(['cdtaCode_entry', 'Station_Readings_Entry_hour']).sum().reset_index()\\\n",
    "    [['cdtaCode_entry', 'Station_Readings_Entry_hour','net_entries']].pivot(index='cdtaCode_entry', columns=\"Station_Readings_Entry_hour\", values=\"net_entries\")\n",
    "entry_hour_count_cdta.columns = [\"Entry_total_trip_count_hour_\" + str(col) for col in entry_hour_count_cdta.columns]\n",
    "\n",
    " ##  Exit Hour count by CDTA\n",
    "exit_hour_count_cdta = hour_exit_stations.groupby(['cdtaCode_exit', 'Station_Readings_Exit_hour']).sum().reset_index()\\\n",
    "    [['cdtaCode_exit', 'Station_Readings_Exit_hour','net_exits']].pivot(index='cdtaCode_exit', columns=\"Station_Readings_Exit_hour\", values=\"net_exits\")\n",
    "exit_hour_count_cdta.columns = [\"Exit_total_trip_count_hour_\" + str(col) for col in exit_hour_count_cdta.columns]\n",
    "\n",
    "#print(exit_hour_count_cdta.shape)\n",
    "#print(entry_hour_count_cdta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Calculate CDTA Weekday wise (Sunday to Saturday ) NetEntries and NetExits\n",
    "\n",
    "print(mta_df.columns)\n",
    "weekday_entry_stations = mta_df.groupby(['Station_Readings_Entry_weekday','station'])[\"net_entries\"].sum().reset_index().copy()\n",
    "weekday_exit_stations = mta_df.groupby(['Station_Readings_Exit_weekday','station'])[\"net_exits\"].sum().reset_index().copy()\n",
    "\n",
    "weekday_entry_stations['cdtaCode_entry'] = weekday_entry_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "weekday_exit_stations['cdtaCode_exit'] = weekday_exit_stations['station'].map(cdta_station_dict).fillna(\"UN99\")\n",
    "\n",
    " ## Entry Weekday count by CDTA\n",
    "entry_weekday_count_cdta = weekday_entry_stations.groupby(['cdtaCode_entry', 'Station_Readings_Entry_weekday']).sum().reset_index()\\\n",
    "    [['cdtaCode_entry', 'Station_Readings_Entry_weekday','net_entries']].pivot(index='cdtaCode_entry', columns=\"Station_Readings_Entry_weekday\", values=\"net_entries\")\n",
    "entry_weekday_count_cdta.columns = [\"Entry_total_trip_count_weekday_\" + str(col) for col in entry_weekday_count_cdta.columns]\n",
    "\n",
    " ## Exit Weekday count by CDTA\n",
    "exit_weekday_count_cdta = weekday_exit_stations.groupby(['cdtaCode_exit', 'Station_Readings_Exit_weekday']).sum().reset_index()\\\n",
    "    [['cdtaCode_exit', 'Station_Readings_Exit_weekday','net_exits']].pivot(index='cdtaCode_exit', columns=\"Station_Readings_Exit_weekday\", values=\"net_exits\")\n",
    "exit_weekday_count_cdta.columns = [\"Exit_total_trip_count_weekday_\" + str(col) for col in exit_weekday_count_cdta.columns]\n",
    "\n",
    "#print(exit_weekday_count_cdta.shape)\n",
    "#print(entry_weekday_count_cdta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdta_subway_df = stationWithCdta.groupby('cdtaCode')[[\"net_entries\",\"net_exits\",\"CDTAName\"]].sum().sort_values(by='CDTAName', ascending=False).reset_index().copy()\n",
    "\n",
    "#Filter CDTAs that do not have any traffic mapped\n",
    "#cdta_subway_df= cdta_subway_df[cdta_subway_df['net_traffic'] != 0]\n",
    "\n",
    "#Map Borough to CDTA DF\n",
    "cdta_subway_df['borough'] = cdta_subway_df[\"cdtaCode\"].apply(lambda x: \"EWR\" if \"EWR\" in x else x[:2]).map(\n",
    "    {\n",
    "        'EWR': 'EWR',\n",
    "        'QN': 'Queens',\n",
    "        'BX': 'Bronx',\n",
    "        'MN': 'Manhattan',\n",
    "        'SI': 'Staten Island',\n",
    "        'BK': 'Brooklyn'\n",
    "    }\n",
    ")\n",
    "\n",
    "#cdta_subway_df['CDTA_name'] = cdta_subway_df['cdtaCode'].map(cdta_dict).fillna(\"\")\n",
    "\n",
    "cdta_subway_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
